{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "char2idx = {c:i for i,c in enumerate(char_arr)}\n",
    "idx2char = {i:c for i,c in enumerate(char_arr)}\n",
    "n_class = len(char2idx)\n",
    "max_seq = max([len(word) for word in np.array(seq_data).reshape(-1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    inputs,outputs,targets = [],[],[]\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i]+ 'P'*(max_seq-len(seq[i]))\n",
    "        \n",
    "        input = [char2idx[k] for k in seq[0]]\n",
    "        output = [char2idx[k] for k in ('S'+seq[1])]\n",
    "        target = [char2idx[k] for k in (seq[1]+'E')]\n",
    "        \n",
    "        inputs.append(np.eye(n_class)[input])\n",
    "        outputs.append(np.eye(n_class)[output])\n",
    "        targets.append(target)\n",
    "    return torch.FloatTensor(inputs),torch.FloatTensor(outputs),torch.LongTensor(targets)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
    "    def forward(self,X):\n",
    "        outputs,(h,c) = self.lstm(X)\n",
    "        return h,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size,input_size)\n",
    "    def forward(self,X,h,c):\n",
    "        # X [batch_size,input_size]\n",
    "        X = X.unsqueeze(1)   #X[batch_size,1,input_size]\n",
    "        outputs,(h,c) = self.lstm(X,(h,c))   #outputs [batch_size,1,hidden_size]\n",
    "        outputs  = outputs.squeeze(1) #outputs [batch_size,hidden_size]\n",
    "        output = self.fc(outputs)\n",
    "        return output,h,c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def  __init__(self,encoder,decoder):\n",
    "        super(Seq2seq,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,src, target,teacher_forcing_ratio=0.5):\n",
    "        #src [batch_size,seq,input_dim]\n",
    "        #target [batch_size,seq,input_dim]\n",
    "        batch_size = target.size()[0]\n",
    "        max_seq = target.size()[1]\n",
    "        input_dim = target.size()[2]\n",
    "        \n",
    "        h,c = self.encoder(src)\n",
    "        outputs = torch.zeros(batch_size,max_seq,input_dim).cuda()\n",
    "        input = target[:,0,:]\n",
    "        for i in range(max_seq):\n",
    "            output,h,c = self.decoder(input,h,c)\n",
    "            outputs[:,i,:] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = target[:,i,:] if teacher_force else output\n",
    "        return outputs\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,outputs,targets = make_batch()\n",
    "dataset = Data.TensorDataset(inputs,outputs,targets)\n",
    "loader = Data.DataLoader(dataset,3,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(n_class,n_hidden,1).cuda()\n",
    "decoder = Decoder(n_class,n_hidden,1).cuda()\n",
    "model = Seq2seq(encoder,decoder).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),1.e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1999,loss = 0.00044\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    for input,output,target in loader:\n",
    "        pred = model(input.cuda(),output.cuda(),0.5)\n",
    "        loss = criterion(pred.view(-1,pred.size(-1)),target.cuda().view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch+1) %10 ==0:\n",
    "            print(\"epoch = %04d,loss = %.5f\"%(epoch,loss),end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'o', 'w', 'n', 'P', 'E']\n",
      "['l', 'o', 'w', 'P', 'P', 'E']\n",
      "['q', 'u', 'e', 'e', 'n', 'E']\n",
      "['w', 'h', 'i', 't', 'e', 'E']\n",
      "['w', 'o', 'm', 'e', 'n', 'E']\n",
      "['b', 'o', 'y', 'P', 'P', 'E']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for input,output,target in loader:\n",
    "    pred = model(input.cuda(),output.cuda(),0)\n",
    "    a=np.argmax(pred.cpu().data,axis=-1)\n",
    "    for l in a:\n",
    "        print([idx2char[int(i)] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['q', 'u', 'e', 'e', 'n', 'E']\n",
      "['w', 'h', 'i', 't', 'e', 'E']\n",
      "['w', 'o', 'm', 'e', 'n', 'E']\n",
      "['b', 'o', 'y', 'P', 'P', 'E']\n",
      "['l', 'o', 'w', 'P', 'P', 'E']\n",
      "['d', 'o', 'w', 'n', 'P', 'E']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for input,output,target in loader:\n",
    "    h,c = model.encoder(input.cuda())\n",
    "    batch_size = output.size()[0]\n",
    "    max_seq = output.size()[1]\n",
    "    input_dim = output.size()[2]\n",
    "    outputs = torch.zeros(batch_size,max_seq,input_dim).cuda()\n",
    "    inp = output[:,0,:].cuda()\n",
    "    for i in range(max_seq):\n",
    "        o,h,c =model.decoder(inp,h,c)\n",
    "        outputs[:,i,:] = o\n",
    "        inp = o\n",
    "    a=np.argmax(outputs.cpu().data,axis=-1)\n",
    "    for l in a:\n",
    "        print([idx2char[int(i)] for i in l])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 128])\n",
      "torch.Size([6, 29])\n",
      "['w', 'o', 'm', 'e', 'n', 'E', 'E']\n",
      "['w', 'h', 'i', 't', 'e', 'E', 'E']\n",
      "['q', 'u', 'e', 'e', 'n', 'E', 'E']\n",
      "['b', 'o', 'y', 'P', 'P', 'E', 'E']\n",
      "['d', 'o', 'w', 'n', 'P', 'E', 'E']\n",
      "['l', 'o', 'w', 'P', 'P', 'E', 'E']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "h,c = model.encoder(inputs.cuda())\n",
    "print(h.size())\n",
    "batch_size = outputs.size()[0]\n",
    "max_seq = outputs.size()[1]\n",
    "input_dim = outputs.size()[2]\n",
    "outputs = torch.zeros(batch_size,max_seq,input_dim).cuda()\n",
    "inp = outputs[:,0,:].cuda()\n",
    "print(inp.size())\n",
    "for i in range(max_seq):\n",
    "    o,h,c =model.decoder(inp,h,c)\n",
    "    outputs[:,i,:] = o\n",
    "    inp = o\n",
    "a=np.argmax(outputs.cpu().data,axis=-1)\n",
    "for l in a:\n",
    "    print([idx2char[int(i)] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
